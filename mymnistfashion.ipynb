{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccc81dd",
   "metadata": {
    "_cell_guid": "1e69e655-9df2-4d22-a7cd-a09bb980bfd6",
    "_uuid": "f5d576ab-4758-4d98-a12b-50fea8bac5d6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-10T17:03:24.145279Z",
     "iopub.status.busy": "2024-11-10T17:03:24.144599Z",
     "iopub.status.idle": "2024-11-10T17:03:24.891584Z",
     "shell.execute_reply": "2024-11-10T17:03:24.890704Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.75406,
     "end_time": "2024-11-10T17:03:24.893852",
     "exception": false,
     "start_time": "2024-11-10T17:03:24.139792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n",
      "/kaggle/input/fashionmnist/fashion-mnist_test.csv\n",
      "/kaggle/input/fashionmnist/fashion-mnist_train.csv\n",
      "/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-images-idx3-ubyte\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e580e62a",
   "metadata": {
    "_cell_guid": "48dcd746-651e-4272-88ff-04537d55f517",
    "_uuid": "2caa03b2-d14d-4795-9dca-e0b89dd269bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-10T17:03:24.901416Z",
     "iopub.status.busy": "2024-11-10T17:03:24.900979Z",
     "iopub.status.idle": "2024-11-10T17:03:28.137924Z",
     "shell.execute_reply": "2024-11-10T17:03:28.137161Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.242981,
     "end_time": "2024-11-10T17:03:28.140076",
     "exception": false,
     "start_time": "2024-11-10T17:03:24.897095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # MaxPool layer after each convolutional layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 1 * 1, 512)   # Adjusted for the new dimensions after pooling\n",
    "        self.fc2 = nn.Linear(512, 256)           # Additional fully connected layer\n",
    "        self.fc3 = nn.Linear(256, 128)           # Additional fully connected layer\n",
    "        self.fc4 = nn.Linear(128, 64)            # Additional fully connected layer\n",
    "        self.fc5 = nn.Linear(64, 10)             # Assuming 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer, ReLU, then MaxPool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        \n",
    "        # Apply second convolutional layer, ReLU, then MaxPool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        \n",
    "        # Apply third convolutional layer, ReLU, then MaxPool\n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        # Apply fourth convolutional layer, ReLU, then MaxPool\n",
    "        x = self.pool(F.relu(self.conv4(x)))  \n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 256 * 1 * 1)  # Adjusted size after pooling\n",
    "        \n",
    "        # Apply fully connected layers with ReLU activations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        # Final output layer\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9166aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:03:28.148008Z",
     "iopub.status.busy": "2024-11-10T17:03:28.147536Z",
     "iopub.status.idle": "2024-11-10T17:03:30.859578Z",
     "shell.execute_reply": "2024-11-10T17:03:30.858775Z"
    },
    "papermill": {
     "duration": 2.718414,
     "end_time": "2024-11-10T17:03:30.861925",
     "exception": false,
     "start_time": "2024-11-10T17:03:28.143511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Assuming grayscale, adjust as needed\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        # The data parameter here is a pandas DataFrame that already contains a split dataset\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the label (first column) and pixel values (remaining columns)\n",
    "        label = int(self.data.iloc[idx, 0])  # Convert label to integer\n",
    "        image = self.data.iloc[idx, 1:].values.astype(np.float32)  # Pixels as float32\n",
    "        image = image.reshape(28, 28)  # Reshape to 28x28 if images are 28x28 pixels\n",
    "\n",
    "        # Convert the image to a PIL Image for the transformation\n",
    "        image = Image.fromarray(image)  # Convert to PIL Image (default mode is 'L' for grayscale)\n",
    "\n",
    "        # Apply transform (e.g., ToTensor)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0346f558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T17:03:30.869716Z",
     "iopub.status.busy": "2024-11-10T17:03:30.869160Z",
     "iopub.status.idle": "2024-11-10T17:24:48.161067Z",
     "shell.execute_reply": "2024-11-10T17:24:48.160021Z"
    },
    "papermill": {
     "duration": 1277.299241,
     "end_time": "2024-11-10T17:24:48.164281",
     "exception": false,
     "start_time": "2024-11-10T17:03:30.865040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100: 100%|██████████| 1500/1500 [00:38<00:00, 39.41it/s, accuracy=78.9, loss=0.576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5751, Accuracy: 78.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1/100: 100%|██████████| 375/375 [00:07<00:00, 47.03it/s, accuracy=85.2, loss=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4151, Validation Accuracy: 85.25%\n",
      "Current learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.09it/s, accuracy=85.9, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.3968, Accuracy: 85.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2/100: 100%|██████████| 375/375 [00:08<00:00, 45.93it/s, accuracy=86.1, loss=0.387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3829, Validation Accuracy: 86.08%\n",
      "Current learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.07it/s, accuracy=87.5, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.3527, Accuracy: 87.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 3/100: 100%|██████████| 375/375 [00:08<00:00, 46.65it/s, accuracy=87.7, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3452, Validation Accuracy: 87.72%\n",
      "Current learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.20it/s, accuracy=88.6, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.3211, Accuracy: 88.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 4/100: 100%|██████████| 375/375 [00:08<00:00, 46.55it/s, accuracy=89.3, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3108, Validation Accuracy: 89.30%\n",
      "Current learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.93it/s, accuracy=89.3, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.3061, Accuracy: 89.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 5/100: 100%|██████████| 375/375 [00:08<00:00, 46.15it/s, accuracy=88.9, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3006, Validation Accuracy: 88.88%\n",
      "Current learning rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.14it/s, accuracy=91.1, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.2469, Accuracy: 91.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 6/100: 100%|██████████| 375/375 [00:08<00:00, 46.54it/s, accuracy=89.3, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2907, Validation Accuracy: 89.26%\n",
      "Current learning rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.20it/s, accuracy=91.5, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.2379, Accuracy: 91.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 7/100: 100%|██████████| 375/375 [00:07<00:00, 47.14it/s, accuracy=89, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3101, Validation Accuracy: 89.02%\n",
      "Current learning rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.40it/s, accuracy=91.6, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.2290, Accuracy: 91.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 8/100: 100%|██████████| 375/375 [00:08<00:00, 46.64it/s, accuracy=90.7, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2632, Validation Accuracy: 90.72%\n",
      "Current learning rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.96it/s, accuracy=92, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.2208, Accuracy: 91.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 9/100: 100%|██████████| 375/375 [00:08<00:00, 45.89it/s, accuracy=90.8, loss=0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2656, Validation Accuracy: 90.83%\n",
      "Current learning rate: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.86it/s, accuracy=92.1, loss=0.216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2152, Accuracy: 92.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 10/100: 100%|██████████| 375/375 [00:07<00:00, 46.98it/s, accuracy=90.8, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2848, Validation Accuracy: 90.81%\n",
      "Current learning rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.80it/s, accuracy=93, loss=0.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.1888, Accuracy: 93.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 11/100: 100%|██████████| 375/375 [00:08<00:00, 46.84it/s, accuracy=91.5, loss=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2432, Validation Accuracy: 91.48%\n",
      "Current learning rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.94it/s, accuracy=93.2, loss=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.1795, Accuracy: 93.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 12/100: 100%|██████████| 375/375 [00:07<00:00, 46.92it/s, accuracy=91.4, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2464, Validation Accuracy: 91.38%\n",
      "Current learning rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.29it/s, accuracy=93.5, loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.1743, Accuracy: 93.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 13/100: 100%|██████████| 375/375 [00:08<00:00, 46.70it/s, accuracy=91.7, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2319, Validation Accuracy: 91.72%\n",
      "Current learning rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.54it/s, accuracy=93.7, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.1703, Accuracy: 93.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 14/100: 100%|██████████| 375/375 [00:08<00:00, 46.41it/s, accuracy=91.8, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2337, Validation Accuracy: 91.77%\n",
      "Current learning rate: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.19it/s, accuracy=93.7, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.1685, Accuracy: 93.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 15/100: 100%|██████████| 375/375 [00:07<00:00, 47.27it/s, accuracy=91.9, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2350, Validation Accuracy: 91.92%\n",
      "Current learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.37it/s, accuracy=94.3, loss=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.1501, Accuracy: 94.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 16/100: 100%|██████████| 375/375 [00:08<00:00, 45.07it/s, accuracy=92.1, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2407, Validation Accuracy: 92.10%\n",
      "Current learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.43it/s, accuracy=94.6, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.1450, Accuracy: 94.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 17/100: 100%|██████████| 375/375 [00:07<00:00, 47.16it/s, accuracy=92.3, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2344, Validation Accuracy: 92.28%\n",
      "Current learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.45it/s, accuracy=94.8, loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.1406, Accuracy: 94.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 18/100: 100%|██████████| 375/375 [00:07<00:00, 47.04it/s, accuracy=92.3, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2440, Validation Accuracy: 92.29%\n",
      "Current learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.74it/s, accuracy=94.8, loss=0.138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.1375, Accuracy: 94.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 19/100: 100%|██████████| 375/375 [00:08<00:00, 45.37it/s, accuracy=92.2, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2421, Validation Accuracy: 92.20%\n",
      "Current learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.99it/s, accuracy=94.8, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.1368, Accuracy: 94.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 20/100: 100%|██████████| 375/375 [00:07<00:00, 47.74it/s, accuracy=92.3, loss=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2374, Validation Accuracy: 92.34%\n",
      "Current learning rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.51it/s, accuracy=95.2, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.1283, Accuracy: 95.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 21/100: 100%|██████████| 375/375 [00:08<00:00, 46.58it/s, accuracy=92.3, loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2470, Validation Accuracy: 92.28%\n",
      "Current learning rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/100: 100%|██████████| 1500/1500 [00:36<00:00, 40.67it/s, accuracy=95.4, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.1221, Accuracy: 95.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 22/100: 100%|██████████| 375/375 [00:07<00:00, 47.96it/s, accuracy=92.5, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2524, Validation Accuracy: 92.46%\n",
      "Current learning rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.53it/s, accuracy=95.4, loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.1220, Accuracy: 95.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 23/100: 100%|██████████| 375/375 [00:08<00:00, 46.03it/s, accuracy=92.7, loss=0.248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2455, Validation Accuracy: 92.65%\n",
      "Current learning rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.41it/s, accuracy=95.5, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.1190, Accuracy: 95.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 24/100: 100%|██████████| 375/375 [00:07<00:00, 47.61it/s, accuracy=92.6, loss=0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2482, Validation Accuracy: 92.59%\n",
      "Current learning rate: 0.000063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25/100: 100%|██████████| 1500/1500 [00:36<00:00, 40.58it/s, accuracy=95.5, loss=0.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.1155, Accuracy: 95.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 25/100: 100%|██████████| 375/375 [00:08<00:00, 46.63it/s, accuracy=92.4, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2622, Validation Accuracy: 92.42%\n",
      "Current learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.36it/s, accuracy=95.8, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.1122, Accuracy: 95.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 26/100: 100%|██████████| 375/375 [00:07<00:00, 47.44it/s, accuracy=92.5, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2516, Validation Accuracy: 92.51%\n",
      "Current learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27/100: 100%|██████████| 1500/1500 [00:37<00:00, 40.44it/s, accuracy=95.8, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.1115, Accuracy: 95.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 27/100: 100%|██████████| 375/375 [00:07<00:00, 47.39it/s, accuracy=92.4, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2467, Validation Accuracy: 92.43%\n",
      "Current learning rate: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28/100: 100%|██████████| 1500/1500 [00:37<00:00, 39.65it/s, accuracy=95.9, loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.1114, Accuracy: 95.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 28/100: 100%|██████████| 375/375 [00:07<00:00, 47.12it/s, accuracy=92.5, loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2492, Validation Accuracy: 92.50%\n",
      "Current learning rate: 0.000031\n",
      "Early stopping triggered: No improvement in validation accuracy for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the CSV data\n",
    "csv_file_path = '/kaggle/input/fashionmnist/fashion-mnist_train.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Split the data into train and validation (80% train, 20% validation)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming you already have the test set, just load it\n",
    "test_data = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n",
    "\n",
    "# Create datasets for train, validation, and test with transform applied\n",
    "train_dataset = CustomImageDataset(data=train_data, transform=transform)\n",
    "val_dataset = CustomImageDataset(data=val_data, transform=transform)\n",
    "test_dataset = CustomImageDataset(data=test_data, transform=transform)\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Assuming your dataset is split into train_loader and val_loader\n",
    "# Define your model, loss function, and optimizer\n",
    "model = SimpleCNN()  # Replace with your actual model\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Common loss for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 5  # Number of epochs to wait for improvement in validation loss\n",
    "best_val_loss = np.inf  # Start with a very large number\n",
    "best_val_accuracy = 0.0  # Start with a low validation accuracy\n",
    "epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Path to save the best model\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100  # Set number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Wrap the training loop with tqdm for progress bar\n",
    "    with tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for inputs, labels in pbar:  # train_loader is your training DataLoader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss for reporting\n",
    "\n",
    "            # For accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update tqdm description with current loss and accuracy\n",
    "            pbar.set_postfix(loss=running_loss / (pbar.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    # Print the training loss and accuracy for this epoch\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Wrap the validation loop with tqdm for progress bar\n",
    "    with tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        with torch.no_grad():  # No gradients needed during evaluation\n",
    "            for inputs, labels in pbar:  # val_loader is your validation DataLoader\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # For accuracy calculation\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Update tqdm description with current validation loss and accuracy\n",
    "                pbar.set_postfix(loss=val_loss / (pbar.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    # Print the validation loss and accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    scheduler.step()  # Adjust the learning rate based on the scheduler\n",
    "\n",
    "    # Optionally, print the current learning rate\n",
    "    print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy  # Update best validation accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save the model with the best accuracy\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "    else:\n",
    "        epochs_without_improvement += 1  # Increment counter\n",
    "\n",
    "    # If no improvement for 'early_stopping_patience' epochs, stop training\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered: No improvement in validation accuracy for 5 epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7aac98c",
   "metadata": {
    "_cell_guid": "d861301b-b27b-4641-90b6-ccb67eb1345a",
    "_uuid": "330e7024-e465-4564-9910-823c369a58ac",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-10T17:24:58.503614Z",
     "iopub.status.busy": "2024-11-10T17:24:58.502988Z",
     "iopub.status.idle": "2024-11-10T17:25:04.866483Z",
     "shell.execute_reply": "2024-11-10T17:25:04.865478Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 11.575881,
     "end_time": "2024-11-10T17:25:04.869529",
     "exception": false,
     "start_time": "2024-11-10T17:24:53.293648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:06<00:00, 49.46it/s, accuracy=92.8, loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2261, Test Accuracy: 92.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best model for testing with weights_only=True\n",
    "best_model = SimpleCNN()  # Initialize the model\n",
    "best_model.load_state_dict(torch.load(best_model_path, weights_only=True))  # Load only the model weights\n",
    "best_model.to(device)  # Move to the correct device\n",
    "\n",
    "\n",
    "# Testing the best model\n",
    "best_model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Wrap the testing loop with tqdm for progress bar\n",
    "with tqdm(test_loader, desc=\"Testing\") as pbar:\n",
    "    with torch.no_grad():  # No gradients needed during evaluation\n",
    "        for inputs, labels in pbar:  # test_loader is your test DataLoader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = best_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # For accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update tqdm description with current test loss and accuracy\n",
    "            pbar.set_postfix(loss=test_loss / (pbar.n + 1), accuracy=100 * correct / total)\n",
    "\n",
    "# Print the test results\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2243,
     "sourceId": 9243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1310.098959,
   "end_time": "2024-11-10T17:25:11.511088",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-10T17:03:21.412129",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
